# Data Warehouse Configuration (read data source, write predictions)
# Note: No environment suffix - single project with simplified dataset names
data_warehouse:
  project_id: bgg-data-warehouse
  location: US
  datasets:
    raw: raw
    core: core
    analytics: analytics
  features_table: games_features
  features_dataset: analytics

# Predictions Destination (landing table - Dataform in bgg-data-warehouse consumes this)
predictions:
  project_id: bgg-predictive-models
  dataset: raw
  table: ml_predictions_landing

# ML Project Configuration
ml_project:
  project_id: bgg-predictive-models
  bucket_name: bgg-predictive-models
  # Models stored at: gs://bgg-predictive-models/{environment}/models/...
  # Predictions at: gs://bgg-predictive-models/{environment}/predictions/...

# Default environment
default_environment: dev

# Year Configuration
# All years are inclusive (e.g., train_through: 2021 means include 2021)
years:
  current: 2025
  recent_year_threshold: 2  # years to exclude from current when finalizing
  training:
    train_through: 2021  # train on data through this year (inclusive)
    tune_start: 2022
    tune_through: 2022
    test_start: 2023
    test_through: 2023
  eval:
    start: 2023
    end: 2023
  score:
    start: 2024    # current - 1
    end: 2024      # current + 4

# Model Configuration
models:
  predictions_dir: "./models/experiments/predictions"
  hurdle:
    type: logistic
    experiment_name: logistic-hurdle
    use_embeddings: true
  complexity:
    type: ard
    experiment_name: ard-ridge-complexity
    use_sample_weights: false
    use_embeddings: true
  rating:
    type: ard
    experiment_name: ard-ridge-rating
    use_sample_weights: false
    min_ratings: 5
    use_embeddings: true
  users_rated:
    type: ard
    experiment_name: ard-ridge-users_rated
    use_sample_weights: false
    min_ratings: 0
    use_embeddings: true
  geek_rating:
    type: ard
    experiment_name: ard-geek_rating

# Evaluate models over time
evaluate:
  output_dir: "./models/experiments"

# Embeddings Configuration
embeddings:
  algorithm: svd  # pca, svd, umap, autoencoder
  embedding_dim: 64
  experiment_name: game-embeddings
  min_ratings: 5  # Minimum users_rated for training data
  algorithms:
    pca:
      whiten: true
    svd:
      n_iter: 5
    autoencoder:
      hidden_layers: [512, 256, 128]
      epochs: 200
      batch_size: 256
      learning_rate: 0.001
      patience: 15
      min_delta: 0.000001
    vae:
      hidden_layers: [512, 256, 128]
      epochs: 150
      batch_size: 256
      learning_rate: 0.001
      beta: 1.0
      patience: 15
      min_delta: 0.000001
  upload:
    # Raw table where new embeddings are written (Dataform pulls from here)
    dataset: raw
    table: game_embeddings
  vector_search:
    # Enriched table for similarity search (embeddings + game features for filtering)
    project: bgg-data-warehouse
    dataset: analytics
    table: game_similarity_search
  search:
    default_distance_type: cosine  # cosine, euclidean, dot_product
    default_top_k: 10

# Text Embeddings Configuration (word embeddings from descriptions)
text_embeddings:
  algorithm: pmi  # pmi, word2vec
  embedding_dim: 100
  experiment_name: text-embeddings
  document_method: sif  # mean, tfidf, sif
  algorithms:
    pmi:
      window_size: 5
      min_count: 5
    word2vec:
      window_size: 5
      min_count: 5
      sg: 1  # 1 for skip-gram, 0 for CBOW
      epochs: 10
  # Upload configuration for BigQuery (same pattern as game_embeddings)
  upload:
    dataset: raw
    table: description_embeddings

# Collection Modeling Configuration
collections:
  storage:
    bucket_name: bgg-predictive-models
    base_prefix: collections
  split:
    negative_sampling_ratio: 5.0
    negative_sampling_strategy: popularity_weighted  # random, popularity_weighted, uniform
    min_ratings_for_negatives: 50
    validation_ratio: 0.15
    test_ratio: 0.15
  model:
    type: lightgbm
    use_sample_weights: false
    handle_imbalance: scale_pos_weight
    threshold_optimization_metric: f2  # f1, f2, precision, recall
    tuning_metric: log_loss
  analysis:
    top_n_recommendations: 100
    top_n_categories: 10
    top_n_mechanics: 10
  min_ratings_for_universe: 25

# Scoring Configuration
scoring:
  models:
    hurdle: hurdle-v2026  # Name of registered model to use
    complexity: complexity-v2026
    rating: rating-v2026
    users_rated: users_rated-v2026
    embeddings: embeddings-v2026
  parameters:
    prior_rating: 5.5
    prior_weight: 2000
  output:
    predictions_path: data/predictions/game_predictions.parquet
