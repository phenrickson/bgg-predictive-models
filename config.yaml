# Data Warehouse Configuration (read data source, write predictions)
# Note: No environment suffix - single project with simplified dataset names
data_warehouse:
  project_id: bgg-data-warehouse
  location: US
  datasets:
    raw: raw
    core: core
    analytics: analytics
  features_table: games_features
  features_dataset: analytics

# Predictions Destination (landing table - Dataform in bgg-data-warehouse consumes this)
predictions:
  project_id: bgg-predictive-models
  dataset: raw
  table: ml_predictions_landing

# ML Project Configuration
ml_project:
  project_id: bgg-predictive-models
  bucket_name: bgg-predictive-models
  # Models stored at: gs://bgg-predictive-models/{environment}/models/...
  # Predictions at: gs://bgg-predictive-models/{environment}/predictions/...

# Default environment
default_environment: dev

# Year Configuration
years:
  current: 2026
  train_end: 2022  # current - 4
  tune_end: 2023   # train_end + 1
  test_start: 2024 # tune_end + 1
  test_end: 2024   # test_start
  eval:
    start: 2018
    end: 2024
  score:
    start: 2025    # current - 1
    end: 2030      # current + 4

# Model Configuration
models:
  hurdle:
    type: lightgbm
    experiment_name: lightgbm-hurdle
  complexity:
    type: catboost
    experiment_name: catboost-complexity
    use_sample_weights: true
    predictions_path: models/experiments/predictions/catboost-complexity.parquet
  rating:
    type: catboost
    experiment_name: catboost-rating
    use_sample_weights: true
    min_ratings: 5
  users_rated:
    type: ridge
    experiment_name: ridge-users_rated
    min_ratings: 0

# evaluate models over time
evaluate:
  # Available model types (like Makefile variables)
  model_types:
    linear: linear
    ridge: ridge
    logistic: logistic
    catboost: catboost
    lightgbm: lightgbm
    lightgbm_linear: lightgbm_linear
  
  # Default model assignments 
  defaults:
    hurdle_model: lightgbm      
    complexity_model: catboost
    rating_model: catboost
    users_rated_model: ridge
  
  # Model-specific settings (like individual Makefile targets)
  settings:
    hurdle:
      # No special settings for hurdle
    complexity:
      use_sample_weights: true
    rating:
      use_sample_weights: true
      min_ratings: 5
    users_rated:
      min_ratings: 0
  
  # Evaluation-specific settings
  output_dir: "./models/experiments"
  min_ratings_threshold: 0

# Embeddings Configuration
embeddings:
  algorithm: umap  # pca, svd, umap, autoencoder
  embedding_dim: 64
  experiment_name: game-embeddings
  min_ratings: 5  # Minimum users_rated for training data
  algorithms:
    pca:
      whiten: true
    svd:
      n_iter: 5
    umap:
      n_neighbors: 15
      min_dist: 0.1
      metric: cosine
    autoencoder:
      hidden_layers: [512, 256, 128]
      epochs: 100
      batch_size: 256
      learning_rate: 0.001
  upload:
    # Raw table where new embeddings are written (Dataform pulls from here)
    dataset: raw
    table: game_embeddings
  vector_search:
    # Enriched table for similarity search (embeddings + game features for filtering)
    project: bgg-data-warehouse
    dataset: analytics
    table: game_similarity_search
  search:
    default_distance_type: cosine  # cosine, euclidean, dot_product
    default_top_k: 10

# Scoring Configuration
scoring:
  models:
    hurdle: hurdle-v2026  # Name of registered model to use
    complexity: complexity-v2026
    rating: rating-v2026
    users_rated: users_rated-v2026
    embeddings: embeddings-v2026
  parameters:
    prior_rating: 5.5
    prior_weight: 2000
  output:
    predictions_path: data/predictions/game_predictions.parquet
