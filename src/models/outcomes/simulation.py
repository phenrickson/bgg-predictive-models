"""Simulation-based uncertainty estimation for Bayesian models.

This module provides functions for sampling from Bayesian model posteriors
and computing prediction intervals with proper uncertainty propagation.

Key features:
- Fast sampling using pre-computed Cholesky decomposition (11x speedup)
- Conditional sampling for dependent models (rating/users_rated depend on complexity)
- Interval coverage metrics for model calibration
"""

from dataclasses import dataclass
from typing import Dict, List, Optional, Tuple

import numpy as np
import pandas as pd
from sklearn.pipeline import Pipeline


@dataclass
class SimulationResult:
    """Results from a model simulation."""

    game_id: int
    game_name: str
    n_samples: int

    # Actual values (may be None if not available)
    actual_complexity: Optional[float]
    actual_rating: Optional[float]
    actual_users_rated: Optional[float]
    actual_geek_rating: Optional[float]

    # Simulation samples (arrays)
    complexity_samples: np.ndarray
    rating_samples: np.ndarray
    users_rated_samples: np.ndarray  # log scale
    geek_rating_samples: np.ndarray

    # Point estimates
    complexity_point: float
    rating_point: float
    users_rated_point: float
    geek_rating_point: float

    @property
    def users_rated_count_samples(self) -> np.ndarray:
        """Users rated in count scale (inverse of log1p transform)."""
        return np.maximum(np.expm1(self.users_rated_samples), 25)

    def percentile(self, arr: np.ndarray, q: float) -> float:
        """Get percentile of samples."""
        return float(np.percentile(arr, q))

    def interval(self, arr: np.ndarray, level: float = 0.90) -> Tuple[float, float]:
        """Get credible interval at given level."""
        lower = (1 - level) / 2 * 100
        upper = (1 + level) / 2 * 100
        return self.percentile(arr, lower), self.percentile(arr, upper)

    def in_interval(
        self, actual: float, arr: np.ndarray, level: float = 0.90
    ) -> bool:
        """Check if actual value falls within credible interval."""
        lower, upper = self.interval(arr, level)
        return lower <= actual <= upper

    def summary(self) -> Dict:
        """Return summary statistics for all outcomes."""
        return {
            "game_id": self.game_id,
            "game_name": self.game_name,
            "complexity": {
                "actual": self.actual_complexity,
                "point": self.complexity_point,
                "median": float(np.median(self.complexity_samples)),
                "mean": float(self.complexity_samples.mean()),
                "std": float(self.complexity_samples.std()),
                "interval_90": self.interval(self.complexity_samples, 0.90),
                "interval_50": self.interval(self.complexity_samples, 0.50),
                "in_interval_90": (
                    self.in_interval(
                        self.actual_complexity, self.complexity_samples, 0.90
                    )
                    if self.actual_complexity is not None
                    else None
                ),
                "in_interval_50": (
                    self.in_interval(
                        self.actual_complexity, self.complexity_samples, 0.50
                    )
                    if self.actual_complexity is not None
                    else None
                ),
            },
            "rating": {
                "actual": self.actual_rating,
                "point": self.rating_point,
                "median": float(np.median(self.rating_samples)),
                "mean": float(self.rating_samples.mean()),
                "std": float(self.rating_samples.std()),
                "interval_90": self.interval(self.rating_samples, 0.90),
                "interval_50": self.interval(self.rating_samples, 0.50),
                "in_interval_90": (
                    self.in_interval(self.actual_rating, self.rating_samples, 0.90)
                    if self.actual_rating is not None
                    else None
                ),
                "in_interval_50": (
                    self.in_interval(self.actual_rating, self.rating_samples, 0.50)
                    if self.actual_rating is not None
                    else None
                ),
            },
            "users_rated": {
                "actual": self.actual_users_rated,
                "point": self.users_rated_point,
                "median": float(np.median(self.users_rated_count_samples)),
                "mean": float(self.users_rated_count_samples.mean()),
                "std": float(self.users_rated_count_samples.std()),
                "interval_90": self.interval(self.users_rated_count_samples, 0.90),
                "interval_50": self.interval(self.users_rated_count_samples, 0.50),
                "in_interval_90": (
                    self.in_interval(
                        self.actual_users_rated, self.users_rated_count_samples, 0.90
                    )
                    if self.actual_users_rated is not None
                    else None
                ),
                "in_interval_50": (
                    self.in_interval(
                        self.actual_users_rated, self.users_rated_count_samples, 0.50
                    )
                    if self.actual_users_rated is not None
                    else None
                ),
            },
            "geek_rating": {
                "actual": self.actual_geek_rating,
                "point": self.geek_rating_point,
                "median": float(np.median(self.geek_rating_samples)),
                "mean": float(self.geek_rating_samples.mean()),
                "std": float(self.geek_rating_samples.std()),
                "interval_90": self.interval(self.geek_rating_samples, 0.90),
                "interval_50": self.interval(self.geek_rating_samples, 0.50),
                "in_interval_90": (
                    self.in_interval(
                        self.actual_geek_rating, self.geek_rating_samples, 0.90
                    )
                    if self.actual_geek_rating is not None
                    else None
                ),
                "in_interval_50": (
                    self.in_interval(
                        self.actual_geek_rating, self.geek_rating_samples, 0.50
                    )
                    if self.actual_geek_rating is not None
                    else None
                ),
            },
        }


def build_full_sigma(model) -> np.ndarray:
    """Build full covariance matrix handling ARDRegression's pruned features.

    ARDRegression may have a smaller sigma_ if features were pruned.
    This reconstructs the full covariance matrix.

    Args:
        model: Fitted Bayesian model with coef_ and sigma_ attributes.

    Returns:
        Full covariance matrix matching coef_ dimensions.
    """
    coef = model.coef_
    sigma = model.sigma_

    if sigma.shape[0] == len(coef):
        return sigma

    n_features = len(coef)
    full_sigma = np.zeros((n_features, n_features))

    if hasattr(model, "lambda_"):
        # ARDRegression case: features with high lambda were pruned
        active_mask = model.lambda_ < getattr(model, "threshold_lambda", np.inf)
        active_indices = np.where(active_mask)[0]
        for i, ai in enumerate(active_indices):
            for j, aj in enumerate(active_indices):
                full_sigma[ai, aj] = sigma[i, j]
    else:
        # Fallback: assume first n features are active
        n_active = sigma.shape[0]
        full_sigma[:n_active, :n_active] = sigma

    return full_sigma


def compute_cholesky(model) -> np.ndarray:
    """Pre-compute Cholesky decomposition of the covariance matrix.

    This avoids recomputing it on every call to multivariate_normal,
    providing ~40x speedup for posterior sampling.

    Args:
        model: Fitted Bayesian model with sigma_ attribute.

    Returns:
        Lower-triangular Cholesky factor L where LL^T = Sigma.
    """
    sigma = build_full_sigma(model)
    # Add small regularization for numerical stability
    return np.linalg.cholesky(sigma + 1e-10 * np.eye(sigma.shape[0]))


def sample_weights_fast(
    coef: np.ndarray,
    cholesky_L: np.ndarray,
    n_samples: int,
    rng: np.random.Generator,
) -> np.ndarray:
    """Sample weight vectors using pre-computed Cholesky decomposition.

    Uses the transformation: w = mu + L @ z, where z ~ N(0, I).
    Much faster than multivariate_normal which recomputes Cholesky each time.

    Args:
        coef: Mean weight vector (model.coef_).
        cholesky_L: Pre-computed Cholesky factor.
        n_samples: Number of samples to draw.
        rng: NumPy random generator.

    Returns:
        Weight samples of shape (n_samples, n_features).
    """
    z = rng.standard_normal((n_samples, len(coef)))
    return coef + z @ cholesky_L.T


def get_scaler_params_for_column(
    pipeline: Pipeline, column_name: str
) -> Tuple[float, float, int]:
    """Extract mean, std, and index for a column from the preprocessor's scaler.

    Args:
        pipeline: Fitted sklearn pipeline with preprocessor step.
        column_name: Name of column to find parameters for.

    Returns:
        Tuple of (mean, std, column_index).

    Raises:
        ValueError: If column not found in features.
    """
    preprocessor = pipeline.named_steps["preprocessor"]
    scaler = preprocessor.named_steps["scaler"]

    feature_names = None
    for name, step in reversed(list(preprocessor.named_steps.items())):
        if name == "scaler":
            continue
        try:
            feature_names = list(step.get_feature_names_out())
            break
        except (AttributeError, TypeError):
            continue

    if feature_names is None:
        raise ValueError("Could not extract feature names from preprocessor")

    if column_name not in feature_names:
        raise ValueError(f"Column {column_name} not found in features")

    idx = feature_names.index(column_name)
    mean = scaler.mean_[idx]
    std = scaler.scale_[idx]

    return mean, std, idx


def sample_posterior(
    pipeline: Pipeline,
    features: pd.DataFrame,
    n_samples: int = 1000,
    include_noise: bool = True,
    random_state: int = 42,
    cholesky_L: np.ndarray = None,
) -> np.ndarray:
    """Sample from a Bayesian model's posterior predictive distribution.

    Args:
        pipeline: Fitted sklearn pipeline with preprocessor and Bayesian model.
        features: Input features as DataFrame.
        n_samples: Number of posterior samples.
        include_noise: Whether to add observation noise (aleatoric uncertainty).
        random_state: Random seed for reproducibility.
        cholesky_L: Pre-computed Cholesky decomposition. If None, computed on the fly.

    Returns:
        Array of shape (n_games, n_samples) with posterior samples.

    Raises:
        ValueError: If model doesn't support posterior sampling (no sigma_).
    """
    preprocessor = pipeline.named_steps["preprocessor"]
    model = pipeline.named_steps["model"]

    if not (hasattr(model, "coef_") and hasattr(model, "sigma_")):
        raise ValueError(
            f"Model {type(model).__name__} does not support posterior sampling"
        )

    X_transformed = preprocessor.transform(features)
    if hasattr(X_transformed, "values"):
        X_transformed = X_transformed.values

    rng = np.random.default_rng(random_state)

    # Use pre-computed Cholesky if provided, otherwise compute
    if cholesky_L is None:
        cholesky_L = compute_cholesky(model)

    weight_samples = sample_weights_fast(model.coef_, cholesky_L, n_samples, rng)

    predictions = X_transformed @ weight_samples.T
    if hasattr(model, "intercept_"):
        predictions += model.intercept_

    if include_noise and hasattr(model, "alpha_"):
        noise_std = 1.0 / np.sqrt(model.alpha_)
        predictions += rng.normal(0, noise_std, size=predictions.shape)

    return predictions


def sample_conditional_on_complexity(
    pipeline: Pipeline,
    features: pd.DataFrame,
    complexity_samples: np.ndarray,
    sample_posterior_weights: bool = True,
    include_noise: bool = True,
    random_state: int = 42,
    cholesky_L: np.ndarray = None,
) -> np.ndarray:
    """Sample from rating/users_rated posterior conditional on complexity samples.

    For each complexity sample, generates a corresponding prediction sample,
    properly propagating uncertainty from complexity to the dependent model.

    Args:
        pipeline: Fitted sklearn pipeline (rating or users_rated model).
        features: Base features (without predicted_complexity).
        complexity_samples: Shape (n_games, n_samples) complexity samples.
        sample_posterior_weights: If True, sample from weight posterior.
        include_noise: If True, add observation noise.
        random_state: Random seed.
        cholesky_L: Pre-computed Cholesky decomposition.

    Returns:
        Predictions of shape (n_games, n_samples).
    """
    preprocessor = pipeline.named_steps["preprocessor"]
    model = pipeline.named_steps["model"]

    n_games, n_samples = complexity_samples.shape
    rng = np.random.default_rng(random_state)

    # Get scaler params for predicted_complexity
    complexity_mean, complexity_std, complexity_idx = get_scaler_params_for_column(
        pipeline, "predicted_complexity"
    )

    # Transform base features once (with placeholder complexity)
    features_with_complexity = features.copy()
    features_with_complexity["predicted_complexity"] = complexity_samples[:, 0]
    X_base = preprocessor.transform(features_with_complexity)
    if hasattr(X_base, "values"):
        X_base = X_base.values

    predictions = np.zeros((n_games, n_samples))

    if sample_posterior_weights and hasattr(model, "sigma_"):
        # Use pre-computed Cholesky if provided
        if cholesky_L is None:
            cholesky_L = compute_cholesky(model)

        weight_samples = sample_weights_fast(model.coef_, cholesky_L, n_samples, rng)

        for i in range(n_samples):
            # Update complexity feature with this sample
            scaled_complexity = (
                complexity_samples[:, i] - complexity_mean
            ) / complexity_std
            X_sample = X_base.copy()
            X_sample[:, complexity_idx] = scaled_complexity

            pred = X_sample @ weight_samples[i]
            if hasattr(model, "intercept_"):
                pred += model.intercept_
            predictions[:, i] = pred

        if include_noise and hasattr(model, "alpha_"):
            noise_std = 1.0 / np.sqrt(model.alpha_)
            predictions += rng.normal(0, noise_std, size=predictions.shape)
    else:
        # Point predictions only (no posterior sampling)
        for i in range(n_samples):
            scaled_complexity = (
                complexity_samples[:, i] - complexity_mean
            ) / complexity_std
            X_sample = X_base.copy()
            X_sample[:, complexity_idx] = scaled_complexity
            predictions[:, i] = model.predict(X_sample)

    return predictions


def compute_geek_rating(
    rating_samples: np.ndarray,
    users_rated_log_samples: np.ndarray,
    prior_rating: float = 5.5,
    prior_weight: float = 2000,
) -> np.ndarray:
    """Compute geek rating using Bayesian average formula.

    Args:
        rating_samples: Rating samples of shape (n_games, n_samples).
        users_rated_log_samples: Log-scale users_rated samples.
        prior_rating: Prior mean rating for Bayesian average.
        prior_weight: Prior weight for Bayesian average.

    Returns:
        Geek rating samples of shape (n_games, n_samples).
    """
    # Convert from log scale to count scale
    users_rated_count = np.maximum(np.expm1(users_rated_log_samples), 25)
    rating_clipped = np.clip(rating_samples, 1, 10)

    geek_rating = (
        (rating_clipped * users_rated_count) + (prior_rating * prior_weight)
    ) / (users_rated_count + prior_weight)

    return geek_rating


def precompute_cholesky(
    complexity_pipeline: Pipeline,
    rating_pipeline: Pipeline,
    users_rated_pipeline: Pipeline,
) -> dict:
    """Pre-compute Cholesky decompositions for all models.

    Call this once at startup, then pass the result to simulate_geek_rating
    for significant speedup (~11x faster sampling).

    Args:
        complexity_pipeline: Fitted complexity model pipeline.
        rating_pipeline: Fitted rating model pipeline.
        users_rated_pipeline: Fitted users_rated model pipeline.

    Returns:
        Dictionary with Cholesky factors keyed by model name.
    """
    return {
        "complexity": compute_cholesky(complexity_pipeline.named_steps["model"]),
        "rating": compute_cholesky(rating_pipeline.named_steps["model"]),
        "users_rated": compute_cholesky(users_rated_pipeline.named_steps["model"]),
    }


def simulate_geek_rating(
    game: pd.DataFrame,
    complexity_pipeline: Pipeline,
    rating_pipeline: Pipeline,
    users_rated_pipeline: Pipeline,
    n_samples: int = 1000,
    prior_rating: float = 5.5,
    prior_weight: float = 2000,
    include_noise: bool = True,
    random_state: int = 42,
    cholesky_cache: dict = None,
) -> SimulationResult:
    """Run full simulation for a single game.

    Simulates the full prediction chain:
    1. Sample complexity from posterior
    2. Sample rating conditional on complexity
    3. Sample users_rated conditional on complexity
    4. Compute geek rating from rating and users_rated samples

    Args:
        game: Single-row DataFrame with game features.
        complexity_pipeline: Fitted complexity model pipeline.
        rating_pipeline: Fitted rating model pipeline.
        users_rated_pipeline: Fitted users_rated model pipeline.
        n_samples: Number of posterior samples.
        prior_rating: Bayesian average prior rating.
        prior_weight: Bayesian average prior weight.
        include_noise: Whether to include observation noise.
        random_state: Random seed for reproducibility.
        cholesky_cache: Pre-computed Cholesky decompositions.

    Returns:
        SimulationResult with all samples and summaries.
    """
    game_id = int(game["game_id"].iloc[0])
    game_name = str(game["name"].iloc[0])

    # Extract actuals
    actual_complexity = (
        float(game["complexity"].iloc[0]) if "complexity" in game.columns else None
    )
    actual_rating = (
        float(game["rating"].iloc[0]) if "rating" in game.columns else None
    )
    actual_users_rated = (
        float(game["users_rated"].iloc[0]) if "users_rated" in game.columns else None
    )

    actual_geek_rating = None
    if actual_rating is not None and actual_users_rated is not None:
        actual_geek_rating = (
            (actual_rating * actual_users_rated) + (prior_rating * prior_weight)
        ) / (actual_users_rated + prior_weight)

    # Get pre-computed Cholesky if available
    complexity_L = cholesky_cache.get("complexity") if cholesky_cache else None
    rating_L = cholesky_cache.get("rating") if cholesky_cache else None
    users_rated_L = cholesky_cache.get("users_rated") if cholesky_cache else None

    # Step 1: Sample complexity
    complexity_samples = sample_posterior(
        complexity_pipeline,
        game,
        n_samples=n_samples,
        include_noise=include_noise,
        random_state=random_state,
        cholesky_L=complexity_L,
    )
    complexity_samples = np.clip(complexity_samples, 1, 5)

    # Step 2: Sample rating conditional on complexity
    rating_samples = sample_conditional_on_complexity(
        rating_pipeline,
        game,
        complexity_samples,
        sample_posterior_weights=True,
        include_noise=include_noise,
        random_state=random_state + 1,
        cholesky_L=rating_L,
    )
    rating_samples = np.clip(rating_samples, 1, 10)

    # Step 3: Sample users_rated conditional on complexity
    users_rated_samples = sample_conditional_on_complexity(
        users_rated_pipeline,
        game,
        complexity_samples,
        sample_posterior_weights=True,
        include_noise=include_noise,
        random_state=random_state + 2,
        cholesky_L=users_rated_L,
    )

    # Step 4: Compute geek rating
    geek_rating_samples = compute_geek_rating(
        rating_samples,
        users_rated_samples,
        prior_rating=prior_rating,
        prior_weight=prior_weight,
    )

    # Point estimates
    complexity_point = float(np.clip(complexity_pipeline.predict(game)[0], 1, 5))

    game_with_complexity = game.copy()
    game_with_complexity["predicted_complexity"] = complexity_point

    rating_point = float(
        np.clip(rating_pipeline.predict(game_with_complexity)[0], 1, 10)
    )
    users_rated_point_log = float(
        users_rated_pipeline.predict(game_with_complexity)[0]
    )
    users_rated_point = max(np.expm1(users_rated_point_log), 25)

    geek_rating_point = (
        (rating_point * users_rated_point) + (prior_rating * prior_weight)
    ) / (users_rated_point + prior_weight)

    return SimulationResult(
        game_id=game_id,
        game_name=game_name,
        n_samples=n_samples,
        actual_complexity=actual_complexity,
        actual_rating=actual_rating,
        actual_users_rated=actual_users_rated,
        actual_geek_rating=actual_geek_rating,
        complexity_samples=complexity_samples.flatten(),
        rating_samples=rating_samples.flatten(),
        users_rated_samples=users_rated_samples.flatten(),
        geek_rating_samples=geek_rating_samples.flatten(),
        complexity_point=complexity_point,
        rating_point=rating_point,
        users_rated_point=users_rated_point,
        geek_rating_point=geek_rating_point,
    )


def simulate_batch(
    games: pd.DataFrame,
    complexity_pipeline: Pipeline,
    rating_pipeline: Pipeline,
    users_rated_pipeline: Pipeline,
    n_samples: int = 1000,
    prior_rating: float = 5.5,
    prior_weight: float = 2000,
    random_state: int = 42,
    cholesky_cache: dict = None,
) -> List[SimulationResult]:
    """Run simulation for multiple games (vectorized).

    Processes all games in parallel using matrix operations rather than
    looping game-by-game, providing significant speedup.

    Args:
        games: DataFrame with multiple games.
        complexity_pipeline: Fitted complexity model pipeline.
        rating_pipeline: Fitted rating model pipeline.
        users_rated_pipeline: Fitted users_rated model pipeline.
        n_samples: Number of posterior samples per game.
        prior_rating: Bayesian average prior rating.
        prior_weight: Bayesian average prior weight.
        random_state: Random seed.
        cholesky_cache: Pre-computed Cholesky decompositions.

    Returns:
        List of SimulationResult, one per game.
    """
    # Pre-compute Cholesky if not provided
    if cholesky_cache is None:
        cholesky_cache = precompute_cholesky(
            complexity_pipeline, rating_pipeline, users_rated_pipeline
        )

    n_games = len(games)

    # Step 1: Sample complexity for ALL games at once (n_games, n_samples)
    complexity_samples = sample_posterior(
        complexity_pipeline,
        games,
        n_samples=n_samples,
        include_noise=True,
        random_state=random_state,
        cholesky_L=cholesky_cache.get("complexity"),
    )
    complexity_samples = np.clip(complexity_samples, 1, 5)

    # Step 2: Sample rating conditional on complexity for ALL games
    rating_samples = sample_conditional_on_complexity(
        rating_pipeline,
        games,
        complexity_samples,
        sample_posterior_weights=True,
        include_noise=True,
        random_state=random_state + 1,
        cholesky_L=cholesky_cache.get("rating"),
    )
    rating_samples = np.clip(rating_samples, 1, 10)

    # Step 3: Sample users_rated conditional on complexity for ALL games
    users_rated_samples = sample_conditional_on_complexity(
        users_rated_pipeline,
        games,
        complexity_samples,
        sample_posterior_weights=True,
        include_noise=True,
        random_state=random_state + 2,
        cholesky_L=cholesky_cache.get("users_rated"),
    )

    # Step 4: Compute geek rating vectorized
    geek_rating_samples = compute_geek_rating(
        rating_samples,
        users_rated_samples,
        prior_rating=prior_rating,
        prior_weight=prior_weight,
    )

    # Point estimates (vectorized)
    complexity_points = np.clip(complexity_pipeline.predict(games), 1, 5)

    games_with_complexity = games.copy()
    games_with_complexity["predicted_complexity"] = complexity_points

    rating_points = np.clip(rating_pipeline.predict(games_with_complexity), 1, 10)
    users_rated_points_log = users_rated_pipeline.predict(games_with_complexity)
    users_rated_points = np.maximum(np.expm1(users_rated_points_log), 25)

    geek_rating_points = (
        (rating_points * users_rated_points) + (prior_rating * prior_weight)
    ) / (users_rated_points + prior_weight)

    # Extract actuals
    actual_complexity = games["complexity"].values if "complexity" in games.columns else [None] * n_games
    actual_rating = games["rating"].values if "rating" in games.columns else [None] * n_games
    actual_users_rated = games["users_rated"].values if "users_rated" in games.columns else [None] * n_games

    # Build results list
    results = []
    for i in range(n_games):
        game_id = int(games["game_id"].iloc[i])
        game_name = str(games["name"].iloc[i]) if "name" in games.columns else ""

        act_complexity = float(actual_complexity[i]) if actual_complexity[i] is not None and not np.isnan(actual_complexity[i]) else None
        act_rating = float(actual_rating[i]) if actual_rating[i] is not None and not np.isnan(actual_rating[i]) else None
        act_users_rated = float(actual_users_rated[i]) if actual_users_rated[i] is not None and not np.isnan(actual_users_rated[i]) else None

        act_geek_rating = None
        if act_rating is not None and act_users_rated is not None:
            act_geek_rating = (
                (act_rating * act_users_rated) + (prior_rating * prior_weight)
            ) / (act_users_rated + prior_weight)

        results.append(SimulationResult(
            game_id=game_id,
            game_name=game_name,
            n_samples=n_samples,
            actual_complexity=act_complexity,
            actual_rating=act_rating,
            actual_users_rated=act_users_rated,
            actual_geek_rating=act_geek_rating,
            complexity_samples=complexity_samples[i],
            rating_samples=rating_samples[i],
            users_rated_samples=users_rated_samples[i],
            geek_rating_samples=geek_rating_samples[i],
            complexity_point=float(complexity_points[i]),
            rating_point=float(rating_points[i]),
            users_rated_point=float(users_rated_points[i]),
            geek_rating_point=float(geek_rating_points[i]),
        ))

    return results


def compute_coverage(
    results: List[SimulationResult], level: float = 0.90
) -> Dict[str, Dict]:
    """Compute interval coverage across multiple games.

    Args:
        results: List of SimulationResult objects.
        level: Coverage level (e.g., 0.90 for 90% intervals).

    Returns:
        Dictionary with coverage statistics for each outcome.
    """
    coverage = {
        "complexity": [],
        "rating": [],
        "users_rated": [],
        "geek_rating": [],
    }

    for r in results:
        s = r.summary()
        level_key = f"in_interval_{int(level * 100)}"
        for metric in coverage.keys():
            if s[metric][level_key] is not None:
                coverage[metric].append(s[metric][level_key])

    return {
        metric: {
            "coverage": sum(vals) / len(vals) if vals else None,
            "n": len(vals),
            "expected": level,
        }
        for metric, vals in coverage.items()
    }


def compute_interval_width(
    results: List[SimulationResult], level: float = 0.90
) -> Dict[str, Dict]:
    """Compute interval width statistics across multiple games.

    Args:
        results: List of SimulationResult objects.
        level: Interval level (e.g., 0.90 for 90% intervals).

    Returns:
        Dictionary with interval width statistics for each outcome.
    """
    widths = {
        "complexity": [],
        "rating": [],
        "users_rated": [],
        "geek_rating": [],
    }

    for r in results:
        s = r.summary()
        level_key = f"interval_{int(level * 100)}"
        for metric in widths.keys():
            low, high = s[metric][level_key]
            widths[metric].append(high - low)

    return {
        metric: {
            "median_width": float(np.median(vals)) if vals else None,
            "mean_width": float(np.mean(vals)) if vals else None,
            "std_width": float(np.std(vals)) if vals else None,
            "n": len(vals),
        }
        for metric, vals in widths.items()
    }


def compute_simulation_metrics(
    results: List[SimulationResult],
) -> Dict[str, Dict]:
    """Compute comprehensive simulation metrics.

    Computes RMSE, MAE, RÂ² using median predictions, plus coverage metrics.

    Args:
        results: List of SimulationResult objects.

    Returns:
        Dictionary with metrics for each outcome.
    """
    metrics = {}

    for outcome in ["complexity", "rating", "users_rated", "geek_rating"]:
        actuals = []
        medians = []
        points = []

        for r in results:
            s = r.summary()
            if s[outcome]["actual"] is not None:
                actuals.append(s[outcome]["actual"])
                medians.append(s[outcome]["median"])
                points.append(s[outcome]["point"])

        if not actuals:
            metrics[outcome] = {"n": 0}
            continue

        actuals = np.array(actuals)
        medians = np.array(medians)
        points = np.array(points)

        # Metrics using simulation median
        rmse_sim = float(np.sqrt(np.mean((actuals - medians) ** 2)))
        mae_sim = float(np.mean(np.abs(actuals - medians)))
        ss_res = np.sum((actuals - medians) ** 2)
        ss_tot = np.sum((actuals - actuals.mean()) ** 2)
        r2_sim = float(1 - ss_res / ss_tot) if ss_tot > 0 else 0.0

        # Metrics using point estimate
        rmse_point = float(np.sqrt(np.mean((actuals - points) ** 2)))
        mae_point = float(np.mean(np.abs(actuals - points)))
        ss_res_point = np.sum((actuals - points) ** 2)
        r2_point = float(1 - ss_res_point / ss_tot) if ss_tot > 0 else 0.0

        metrics[outcome] = {
            "n": len(actuals),
            "rmse_sim": rmse_sim,
            "mae_sim": mae_sim,
            "r2_sim": r2_sim,
            "rmse_point": rmse_point,
            "mae_point": mae_point,
            "r2_point": r2_point,
        }

    # Add coverage metrics
    coverage_90 = compute_coverage(results, level=0.90)
    coverage_50 = compute_coverage(results, level=0.50)

    for outcome in ["complexity", "rating", "users_rated", "geek_rating"]:
        if outcome in metrics and metrics[outcome].get("n", 0) > 0:
            metrics[outcome]["coverage_90"] = coverage_90[outcome]["coverage"]
            metrics[outcome]["coverage_50"] = coverage_50[outcome]["coverage"]

    # Add interval width metrics
    width_90 = compute_interval_width(results, level=0.90)
    width_50 = compute_interval_width(results, level=0.50)

    for outcome in ["complexity", "rating", "users_rated", "geek_rating"]:
        if outcome in metrics and metrics[outcome].get("n", 0) > 0:
            metrics[outcome]["interval_width_90"] = width_90[outcome]["median_width"]
            metrics[outcome]["interval_width_50"] = width_50[outcome]["median_width"]

    return metrics
